---
title: python多线程同步
date: 2019-12-29 22:12:04
urlname: python_concurrent_lock
tags: python多线程
---

> **背景**: 项目中有个需求，在某一个api请求的处理逻辑中，将收到的数据信息写入文件，每次写入一条。在并发情况下存在文件读写覆盖问题，因此需要对写操作进行互斥。

<!--more-->

## 1.问题复现
### 1.1 复现工具：
- apache jmeter（推荐）
- postman runer
### 1.2 复现方法
- 同一时间段发送100次请求
    - 例如：10秒中之内发送100次请求
- 瞬时发送100次请求
    - 例如：无延迟发送100次请求
请求处理完成，统计文件中的数据条数

### 1.3 复现结果
为方便统计，简单用一个python脚本做为统计：

#### 1.3.1 统计脚本
```Python
# -*- encoding:utf-8 -*-
  
if __name__ == "__main__":
    with open('example_file', 'r') as f:
        items = f.readlines()
 
    print("数据条数:{num}".format(num=len(items)))
```
#### 1.3.2 结果
conslusion：数据条数均小于100

## 2.解决方案
在并发情况下，文件是作为一种竞争资源进行访问，因此对文件的写操作需要放在临界区，进入临界区的线程才有资格对文件进行实际的写操作。提供互斥访问文件方式，提供临界区的方式在众多变成语言中都提供了锁的方式，下面看看python中有哪些具体的实现方式能够满足需求。

### 2.1 threading.lock
#### 2.1.1 原理
python提供的线程间同步机制，在访问临界区资源的过程中，通过加锁的方式实现互斥访问，没有获取锁的线程将被阻塞，直至当前获取锁的线程完成机器信息写入、释放锁。

#### 2.1.2 实现方式
```Python
import threading
mutex = threading.Lock()
file_path = "example_file"

def create(self, request): # 请求处理方法
    body = json.loeads(request.body)
    register_item(body)
    pass

def register_item(item):
    with mutex:
        with open(file_path, 'a') as f:
            f.write(json.dumps(item))
            f.write('\n')        
```
#### 2.1.3 缺陷
在单纯的多线程情况下，这种互斥加锁的访问方式能够解决同步的问题，缺点这种互斥是阻塞的，并发请求过大的情况下，会造成过多的线程处于阻塞状态，系统的多线程模型可能会出现意想不到的错误提示。<br>

此外，项目的运行方式并不是简单的多线程，而是多进程+多线程模式，threading.Locak()只能在一个进程孵化的子线程之间实现资源的互斥访问，但是对不同进程之间并不能够做约束，所以无法满足项目实际需求。

### 2.2 fcntl.flock
#### 2.2.1 原理
当涉及到多个进程向同一个文件write(或者read)的情况，如果几个进程同时都对这个文件进行写操作，那么文件的内容就会变得非常混乱，这个时候文件锁就派上用场了，python中的文件锁，可以保证同时只有一个进程写文件，目前使用的是fcntl这个库，它实际上为 Unix上的ioctl，flock和fcntl 函数提供了一个接口。python通过调用fcntl.flock()函数对文件加锁。

常用锁类型
- LOCK_SH： 表示要创建一个共享锁，在任意时间内，一个文件的共享锁可以被多个进程拥有
- LOCK_EX： 表示创建一个排他锁，在任意时间内，一个文件的排他锁只能被一个进程拥有

操作注意点
1. 对于文件的 close() 操作会使文件锁失效；
2. 同理，进程结束后文件锁失效；
3. flock() 的 LOCK_EX是“劝告锁”，系统内核不会强制检查锁的状态，需要在代码中进行文件操作的地方显式检查才能生效。（一个进程中使用了acquire显示检查，另一个进程没有进行flock检查，可以直接读写文件）

#### 2.2.2 实现方式
```Python
import fcntl
import threading

file_path = "example_file"

def create(self, request):
        body = json.loads(request.body)
        register_item(body)

def register_item(item):
    with open(file_path, 'a') as f: # 文件关闭，锁也自动释放
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)  # fileno 获取的是打开文件的文件描述符
        f.write(json.dumps(item))
        f.write('\n')
```

## 3. 结论
并发情况下对文件的读写操作，需要结合项目实际的需求，多线程和多进程的运行方式入手，划出临界区，选择合适的并发控制方案。此外，可以了解jmeter 工具做个并发测试，能够更深入理解其中的过程，包括处理的时间，线程池的工作方式。
TODO ：
- GIL 了解
- multiprocessing 了解
- 对比Java中的加锁方式
- 对比python中的线程池和Java中的线程池

